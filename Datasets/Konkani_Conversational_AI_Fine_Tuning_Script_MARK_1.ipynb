{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Konkani Conversational AI Fine-Tuning Script\n",
        "# ---------------------------------------------\n",
        "# This script demonstrates how to fine-tune a pre-trained transformer model\n",
        "# for conversational tasks in Konkani using an English-Konkani dataset.\n",
        "# It uses the Hugging Face libraries (transformers, datasets) and PyTorch.\n",
        "\n",
        "# Step 1: Install necessary libraries\n",
        "# -----------------------------------\n",
        "# Before running, make sure you have these libraries installed:\n",
        "# pip install torch transformers datasets pandas sentencepiece accelerate\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "from datasets import Dataset as HfDataset\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# --- Please edit these variables to match your setup ---\n",
        "\n",
        "# File and Data Configuration\n",
        "FILE_PATH = 'final_dataset.csv'  # <-- IMPORTANT: Change this to the exact path of your CSV file.\n",
        "INPUT_COLUMN = 'ENGLISH'         # <-- The column with the source language (English)\n",
        "TARGET_COLUMN = 'KONKANI'        # <-- The column with the target language (Konkani)\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = 't5-small'          # We use 't5-small' for a balance of performance and training speed.\n",
        "                                 # For higher quality, consider 't5-base' or 't5-large' (requires more GPU memory).\n",
        "PREFIX = \"chat in konkani: \"     # T5 models are trained with prefixes. This tells the model what task to perform.\n",
        "\n",
        "# Training Configuration\n",
        "OUTPUT_DIR = './konkani_t5_model' # Directory to save the fine-tuned model and tokenizer.\n",
        "EPOCHS = 5                       # Number of times to train on the entire dataset.\n",
        "BATCH_SIZE = 8                   # Number of examples per training step. Adjust based on your GPU memory.\n",
        "MAX_INPUT_LENGTH = 128           # Max length for input tokens.\n",
        "MAX_TARGET_LENGTH = 128          # Max length for output tokens.\n",
        "\n",
        "# --- END OF CONFIGURATION ---\n",
        "\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"\n",
        "    Loads the dataset from the CSV file and prepares it for the model.\n",
        "    It handles potential file errors and formats the data into a\n",
        "    Hugging Face Dataset object.\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to load data from: {FILE_PATH}\")\n",
        "    try:\n",
        "        # Load the dataset using pandas, specifying the encoding\n",
        "        df = pd.read_csv(FILE_PATH, encoding='latin1')\n",
        "\n",
        "        # Verify that the required columns exist\n",
        "        if INPUT_COLUMN not in df.columns or TARGET_COLUMN not in df.columns:\n",
        "            raise ValueError(\n",
        "                f\"CSV must contain '{INPUT_COLUMN}' and '{TARGET_COLUMN}' columns. \"\n",
        "                f\"Found columns: {df.columns.tolist()}\"\n",
        "            )\n",
        "        print(\"Successfully loaded the dataset.\")\n",
        "        print(f\"Dataset has {len(df)} rows.\")\n",
        "\n",
        "        # Drop any rows with missing values in our target columns\n",
        "        df.dropna(subset=[INPUT_COLUMN, TARGET_COLUMN], inplace=True)\n",
        "        df = df.astype(str) # Ensure all data is string type\n",
        "\n",
        "        # Convert the pandas DataFrame to a Hugging Face Dataset object\n",
        "        return HfDataset.from_pandas(df)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"---\")\n",
        "        print(f\"ERROR: The file '{FILE_PATH}' was not found.\")\n",
        "        print(\"Please make sure the file path in the script is correct.\")\n",
        "        print(\"---\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the data: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def preprocess_data(dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenizes the input and target text. The T5 model requires a specific\n",
        "    prefix to understand the task, which we add here.\n",
        "    \"\"\"\n",
        "    print(\"Preprocessing and tokenizing data...\")\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        # Prepend the task-specific prefix to the input\n",
        "        inputs = [PREFIX + doc for doc in examples[INPUT_COLUMN]]\n",
        "\n",
        "        # Tokenize the inputs\n",
        "        model_inputs = tokenizer(\n",
        "            inputs,\n",
        "            max_length=MAX_INPUT_LENGTH,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "        # Tokenize the targets (labels)\n",
        "        labels = tokenizer(\n",
        "            text_target=examples[TARGET_COLUMN],\n",
        "            max_length=MAX_TARGET_LENGTH,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "\n",
        "    # Apply the tokenization function to the entire dataset\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "    print(\"Data preprocessing complete.\")\n",
        "    return tokenized_dataset\n",
        "\n",
        "\n",
        "def train_model(tokenized_dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Sets up the model, training arguments, and trainer, then\n",
        "    initiates the fine-tuning process.\n",
        "    \"\"\"\n",
        "    print(\"Setting up the model and trainer...\")\n",
        "\n",
        "    # Load the pre-trained T5 model\n",
        "    model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # Define the training arguments\n",
        "    # These arguments control various aspects of the training run\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE, # If you add an evaluation set\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        save_total_limit=2, # Only keep the best 2 checkpoints\n",
        "        predict_with_generate=True,\n",
        "        report_to=\"none\", # --- THIS LINE WAS ADDED --- Disables wandb logging\n",
        "    )\n",
        "\n",
        "    # The data collator is responsible for creating batches of data.\n",
        "    # It also handles dynamic padding.\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer=tokenizer,\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    # Instantiate the trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        # You can add an eval_dataset here for metrics during training\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    print(\"--- Starting Model Training ---\")\n",
        "    print(f\"Model: {MODEL_NAME}\")\n",
        "    print(f\"Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE}\")\n",
        "    print(f\"Output will be saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "    # Start the training\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"--- Training Complete ---\")\n",
        "\n",
        "    # Save the final model and tokenizer\n",
        "    print(f\"Saving the fine-tuned model to {OUTPUT_DIR}...\")\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "\n",
        "def run_conversation_loop():\n",
        "    \"\"\"\n",
        "    Loads the fine-tuned model from disk and starts a conversational\n",
        "    loop to interact with it.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Konkani Chatbot ---\")\n",
        "\n",
        "    # Check if the model directory exists\n",
        "    if not os.path.exists(OUTPUT_DIR):\n",
        "        print(f\"Error: Model directory '{OUTPUT_DIR}' not found.\")\n",
        "        print(\"Please train the model first by running the main script.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loading fine-tuned model from {OUTPUT_DIR}...\")\n",
        "    try:\n",
        "        tokenizer = T5Tokenizer.from_pretrained(OUTPUT_DIR)\n",
        "        model = T5ForConditionalGeneration.from_pretrained(OUTPUT_DIR)\n",
        "        # Move model to GPU if available\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        print(f\"Model loaded successfully on device: {device}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load the model: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nChatbot is ready. Type your message in English.\")\n",
        "    print(\"Type 'exit' or 'quit' to end the conversation.\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    while True:\n",
        "        english_prompt = input(\"You (English): \")\n",
        "        if english_prompt.lower() in ['exit', 'quit']:\n",
        "            print(\"Bot: Adeus! (Goodbye!)\")\n",
        "            break\n",
        "\n",
        "        # Prepare the input for the model\n",
        "        full_prompt = PREFIX + english_prompt\n",
        "        input_ids = tokenizer.encode(full_prompt, return_tensors='pt').to(device)\n",
        "\n",
        "        # Generate the response from the model\n",
        "        # We use different parameters here to encourage more creative responses\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_length=MAX_TARGET_LENGTH,\n",
        "            num_beams=5, # Beam search can produce better results\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2 # Prevents the model from repeating itself\n",
        "        )\n",
        "\n",
        "        # Decode the generated tokens into a string\n",
        "        konkani_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        print(f\"Bot (Konkani): {konkani_response}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This block runs when the script is executed directly\n",
        "\n",
        "    # Ask the user what they want to do\n",
        "    print(\"Welcome to the Konkani AI Model Trainer.\")\n",
        "    print(\"1: Train a new model\")\n",
        "    print(\"2: Chat with an existing model\")\n",
        "    choice = input(\"Please enter your choice (1 or 2): \")\n",
        "\n",
        "    if choice == '1':\n",
        "        # --- Main Training Pipeline ---\n",
        "        # 1. Load data\n",
        "        raw_dataset = load_and_prepare_data()\n",
        "\n",
        "        if raw_dataset:\n",
        "            # 2. Initialize tokenizer\n",
        "            tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "            # 3. Preprocess and tokenize data\n",
        "            tokenized_dataset = preprocess_data(raw_dataset, tokenizer)\n",
        "\n",
        "            # 4. Train the model\n",
        "            train_model(tokenized_dataset, tokenizer)\n",
        "\n",
        "            # 5. Offer to chat with the newly trained model\n",
        "            chat_now = input(\"Training complete. Would you like to chat with the new model? (yes/no): \")\n",
        "            if chat_now.lower() == 'yes':\n",
        "                run_conversation_loop()\n",
        "\n",
        "    elif choice == '2':\n",
        "        # --- Run Inference ---\n",
        "        run_conversation_loop()\n",
        "    else:\n",
        "        print(\"Invalid choice. Please run the script again and enter 1 or 2.\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "5w7Q7oHYTCOz"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}